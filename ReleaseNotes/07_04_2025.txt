
Features:

Bug Fixes/Re-organization:

Samples:

IdeaDRIP:

	- Understanding Exact Dynamic Programming through Bellman Operators - Greedy Policy from Optimal VF is an #3 (1-5)
	- Understanding Exact Dynamic Programming through Bellman Operators - Value Functions as Vectors #4 (6-15)
	- Understanding Exact Dynamic Programming through Bellman Operators - Expected Rewards and State Transition Probabilities #4 (16-23)
	- Understanding Exact Dynamic Programming through Bellman Operators - B_PI and B_STAR #4 (24-37)
	- Understanding Exact Dynamic Programming through Bellman Operators - Contraction and Monotonicity of #4 (38-45)
	- Understanding Exact Dynamic Programming through Bellman Operators - Policy Evaluation #4 (46-49)
	- Understanding Exact Dynamic Programming through Bellman Operators - Policy Improvement #4 (50-59)
	- Understanding Exact Dynamic Programming through Bellman Operators - Policy Iteration #4 (60-65)
	- Understanding Exact Dynamic Programming through Bellman Operators - Value Iteration #4 (66-69)
	- Understanding Exact Dynamic Programming through Bellman Operators - Greedy Policy from Optimal Value Functionis an Optimal Policy #4 (70-77)
	- Understanding Exact Dynamic Programming through Bellman Operators - Value Functions as Vectors #5 (78-87)
	- Understanding Exact Dynamic Programming through Bellman Operators - Expected Rewards and State Transition Probabilities #5 (88-95)
	- Understanding Exact Dynamic Programming through Bellman Operators - B_PI and B_STAR #5 (96-109)
	- Understanding Exact Dynamic Programming through Bellman Operators - Contraction and Monotonicity of #5 (110-117)
	- Understanding Exact Dynamic Programming through Bellman Operators - Policy Evaluation #5 (118-120)
